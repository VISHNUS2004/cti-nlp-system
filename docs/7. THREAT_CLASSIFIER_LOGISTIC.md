# Threat Classifier (Logistic Regression with TF-IDF)

## Overview

This model is designed to classify **cyber threat descriptions** into predefined categories (e.g., malware, phishing, ransomware).
It uses **TF-IDF vectorization** to transform text into numerical features, followed by a **Logistic Regression** classifier for prediction.

## Why Logistic Regression for Threat Classification?

### 1. **Proven Performance on Text Data**

- Logistic Regression is a **linear model** that works exceptionally well for **high-dimensional, sparse datasets** — exactly the kind of data produced by TF-IDF.
- Many **text classification benchmarks** show that Logistic Regression can outperform more complex models when data is well-preprocessed.

### 2. **Fast and Efficient**

- **Low training time** compared to deep learning models.
- Can handle **large datasets** efficiently without heavy GPU requirements.
- Ideal for **real-time prediction** in the backend.

### 3. **Robust for Imbalanced Data**

- Supports **class weighting**, which can help balance underrepresented threat categories.

## How It Works

### Step 1 — **Text Vectorization (TF-IDF)**

- **TF-IDF (Term Frequency – Inverse Document Frequency)** converts text into a numeric feature matrix.
- Assigns **higher weights** to terms that are frequent in a specific document but rare across the dataset.
- Example:

  - `"malware attack detected"` → `[0.23, 0.0, 0.41, ...]`

### Step 2 — **Classification with Logistic Regression**

- Logistic Regression learns a **set of weights for each term** to predict the probability of each threat category.

- Decision is made by:

  $$
  P(y = c) = \frac{1}{1 + e^{-(w \cdot x + b)}}
  $$

- Where:

  - `w` = learned weights for each term
  - `x` = TF-IDF feature vector
  - `b` = bias term

### Step 3 — **Prediction**

- The model outputs a **probability distribution** across threat categories.
- The **category with the highest probability** is selected as the final prediction.

## Why This Works Better Than Some Traditional Models

| Feature                    | Traditional Keyword Matching | Logistic Regression (TF-IDF) |
| -------------------------- | ---------------------------- | ---------------------------- |
| Handles Synonyms           | No                           | Yes (learns from context)    |
| Generalizes to New Phrases | No                           | Yes                          |
| Learns Feature Importance  | No                           | Yes                          |
| Handles Large Vocab        | Limited                      | Efficient with Sparse Data   |

## Benefits for Our Project

- **High Accuracy** on cleaned cybersecurity text data.
- **Explainability**: We can inspect top-weighted keywords per category.
- **Scalability**: Easily retrained as new threat categories or data arrive.
- **Integration**: Saved as `.pkl` files for direct use in our Flask/FastAPI backend.

---

## Model Files Generated

- `models/tfidf_vectorizer.pkl` → Stores the TF-IDF vocabulary and transformations.
- `models/threat_classifier.pkl` → Stores the trained Logistic Regression model.

---

## Next Improvements

- Add **n-grams (bigrams/trigrams)** to capture short phrases like `"phishing email"`.
- Use **class balancing** to handle rare threat categories.
- Experiment with **Ensemble methods** (Logistic + XGBoost) for edge cases.
